{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import os \n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/ccorbin/.config/gcloud/application_default_credentials.json' \n",
    "os.environ['GCLOUD_PROJECT'] = 'mining-clinical-decisions' \n",
    "%load_ext google.cloud.bigquery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client=bigquery.Client()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in MultiLabel Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cohort = \"\"\"select * from traige_TE.triage_cohort_adjusted_multilabel\"\"\"\n",
    "query_job = client.query(q_cohort)\n",
    "df_cohort = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cohort[['label_24hr', 'label_12hr', 'acute_to_critical_24hr', 'acute_to_critical_12hr']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Predictions From LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_full = pd.read_csv('/home/ccorbin/triage_models/lightgbm/test_yhats.csv')\n",
    "df_preds_simple = pd.read_csv('/home/ccorbin/BMI212/notebooks/results_gmb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_preds_full.shape)\n",
    "print(df_preds_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_simple = df_preds_simple.rename(columns={'preds_gbm' : 'predictions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_full = df_preds_full[['pat_enc_csn_id_coded', 'label', 'predictions']]\n",
    "df_preds_simple = df_preds_simple[['pat_enc_csn_id_coded', 'label', 'predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_full = pd.merge(df_preds_full, df_cohort, on='pat_enc_csn_id_coded', how='left')\n",
    "df_preds_simple = pd.merge(df_preds_simple, df_cohort, on='pat_enc_csn_id_coded', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_full.head()\n",
    "\n",
    "# for i in range(len(df_preds)):\n",
    "#     assert df_preds['label'].values[i] == df_preds['label_24hr'].values[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of patient ids that were used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_jc_uids = set(df_cohort[df_cohort['admit_time'] < '2017-07-01']['jc_uid'].values)\n",
    "test_jc_uids = set(df_preds_full['jc_uid'].values)\n",
    "seen_jc_uids = training_jc_uids.intersection(test_jc_uids)\n",
    "\n",
    "df_preds_unseen = df_preds_full[~df_preds_full['jc_uid'].isin(seen_jc_uids)]\n",
    "\n",
    "# Sanity Check\n",
    "for i in range(len(df_preds_unseen)):\n",
    "    assert df_preds_unseen['jc_uid'].values[i] not in training_jc_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def plot_perf(labels, predictions, title):\n",
    "    # Plots ROC AND PRC side by side\n",
    "    sns.set(font_scale=2.)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig1 = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax1 = fig1.add_subplot(1, 2, 1)\n",
    "\n",
    "\n",
    "    tpr, fpr, _ = roc_curve(labels, predictions)\n",
    "    auroc = roc_auc_score(labels, predictions)\n",
    "\n",
    "    ax1.plot(tpr, fpr, sns.xkcd_rgb[\"denim blue\"], lw=2.5, label = 'AUROC %.2f' % auroc )\n",
    "    ax1.plot([0, 1],\n",
    "             [0, 1],\n",
    "             sns.xkcd_rgb[\"slate grey\"],\n",
    "            lw=1.5, linestyle='--')\n",
    "    \n",
    "    ax1.set_ylabel('Sensitivity')\n",
    "    ax1.set_xlabel('1 - Specificity')\n",
    "    ax1.set_title(title)\n",
    "    ax1.legend()\n",
    "    ax1.plot()\n",
    "    \n",
    "    ax2 = fig1.add_subplot(1, 2, 2)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    auprc = average_precision_score(labels, predictions)\n",
    "\n",
    "    ax2.plot(recall, precision, sns.xkcd_rgb[\"denim blue\"], lw=2.5, label = 'AUPRC %.2f' % auprc)\n",
    "    \n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_title(title)\n",
    "    ax2.legend()\n",
    "    ax2.plot()\n",
    "    \n",
    "\n",
    "plot_perf(df_preds_full['label'], df_preds_full['predictions'], 'All Validation Examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perf(df_preds_simple['label'], df_preds_simple['predictions'], 'All Test Examples Simple Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prob thresh \n",
    "precision, recall, p_thresh = precision_recall_curve(df_preds_full['label'], df_preds_full['predictions'])\n",
    "def get_index_at_recall(val):\n",
    "    for i, rec in enumerate(recall):\n",
    "        if rec < val:\n",
    "            return i-1\n",
    "    return None\n",
    "\n",
    "print(p_thresh[get_index_at_recall(0.8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Performance For Patients that Were Not Seen In Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perf(df_preds_unseen['label'], df_preds_unseen['predictions'], 'New Patients Only')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Performance Where Labels are 0 if acute care only and 1 if moved TO ICU within 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_move = df_preds[(df_preds['label'] == 0) | (df_preds['acute_to_critical_24hr']  == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_move.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perf(df_preds_move['label'], df_preds_move['predictions'], 'Under Triaged Label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "74/len(df_preds_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Get Sensitivity and Specificity Cutoff For Doctors Assuming 24 hours after is correct label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds['init_assignmet'] = df_preds.apply(lambda x: 1 if x.label_24hr and not x.acute_to_critical_24hr else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_preds['label_24hr'], df_preds['init_assignmet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf_with_marker(labels, predictions, title):\n",
    "    # Plots ROC AND PRC side by side\n",
    "    sns.set(font_scale=2.)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig1 = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax1 = fig1.add_subplot(1, 2, 1)\n",
    "\n",
    "\n",
    "    tpr, fpr, _ = roc_curve(labels, predictions)\n",
    "    auroc = roc_auc_score(labels, predictions)\n",
    "\n",
    "    ax1.plot(tpr, fpr, sns.xkcd_rgb[\"denim blue\"], lw=2.5, label = 'AUROC %.2f' % auroc )\n",
    "    ax1.plot([0, 1],\n",
    "             [0, 1],\n",
    "             sns.xkcd_rgb[\"slate grey\"],\n",
    "            lw=1.5, linestyle='--')\n",
    "    \n",
    "    ax1.scatter(np.array([0]), np.array([0.85]), marker='x', s=200, color=sns.xkcd_rgb['light orange'], label = 'ER Doc Decision')\n",
    "    ax1.set_ylabel('Sensitivity')\n",
    "    ax1.set_xlabel('1 - Specificity')\n",
    "    ax1.set_title(title)\n",
    "    ax1.legend()\n",
    "    ax1.plot()\n",
    "    \n",
    "    \n",
    "    ax2 = fig1.add_subplot(1, 2, 2)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    auprc = average_precision_score(labels, predictions)\n",
    "    ax2.scatter(np.array([0.85]), np.array([1]), marker='x', s=200, color=sns.xkcd_rgb['light orange'], label = 'ER Doc Decision')\n",
    "\n",
    "    ax2.plot(recall, precision, sns.xkcd_rgb[\"denim blue\"], lw=2.5, label = 'AUPRC %.2f' % auprc)\n",
    "    \n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_title(title)\n",
    "    ax2.legend()\n",
    "    ax2.plot()\n",
    "    \n",
    "\n",
    "plot_perf_with_marker(df_preds['label'], df_preds['predictions'], 'All Validation Examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
