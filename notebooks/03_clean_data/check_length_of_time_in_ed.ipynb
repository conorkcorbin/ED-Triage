{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the distribution of length of stay in the ER\n",
    "\n",
    "Grab ADT data for patients in our adjusted_cohort and check out how long they stayed in the ER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query database\n",
    "\n",
    "Grab ADT columns for our adjusted cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery adjusted_cohort_adt\n",
    "select jc_uid, pat_enc_csn_id_coded, \n",
    "  pat_class, seq_num_in_enc, effective_time_jittered_utc, pat_lv_of_care\n",
    "from starr_datalake2018.adt\n",
    "where pat_enc_csn_id_coded in \n",
    "  (select distinct pat_enc_csn_id_coded from traige_TE.triage_cohort_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save data to csv\n",
    "adj_cohort_adt_file = \"adjusted_cohort_adt.csv\"\n",
    "adjusted_cohort_adt.to_csv(\"adjusted_cohort_adt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique csn's\n",
    "adjusted_cohort_adt.pat_enc_csn_id_coded.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if CSV file is already saved to system\n",
    "\n",
    "Read in the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the cohort after saving the first time\n",
    "adj_cohort_adt_file = \"adjusted_cohort_adt.csv\"\n",
    "adjusted_cohort_adt = pd.read_csv(adj_cohort_adt_file)\n",
    "\n",
    "# change the effective time to datetime since read in from csv\n",
    "adjusted_cohort_adt.effective_time_jittered_utc = pd.to_datetime(adjusted_cohort_adt.effective_time_jittered_utc)\n",
    "\n",
    "# sort by csn and time\n",
    "adjusted_cohort_adt.sort_values(['pat_enc_csn_id_coded', 'seq_num_in_enc'], inplace=True)\n",
    "\n",
    "# use this to hide ID columns from view\n",
    "hidecols = ['jc_uid', 'pat_enc_csn_id_coded']\n",
    "adjusted_cohort_adt.drop(hidecols, axis=1).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the time between the first Inpatient Event and the first Emergency Services event for each CSN\n",
    "\n",
    "This does not use the admit time that we are using. We will check that afterwards and compare to this.\n",
    "\n",
    "Use some of Conor's code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up variables to use conor's code\n",
    "df_change = adjusted_cohort_adt\n",
    "inpatient_ids = set(adjusted_cohort_adt.pat_enc_csn_id_coded.values)\n",
    "\n",
    "# conor's code\n",
    "df_admit_times = df_change[df_change['pat_enc_csn_id_coded'].isin(inpatient_ids)]\n",
    "df_admit_times = df_admit_times[df_admit_times['pat_class'] == 'Inpatient'].groupby(\n",
    "     'pat_enc_csn_id_coded').first().reset_index()[['pat_enc_csn_id_coded', 'effective_time_jittered_utc']].rename(\n",
    "     columns={'effective_time_jittered_utc' : 'admit_time_jittered'})\n",
    "df_admit_times\n",
    "\n",
    "# Changed this time_since_admit to be the opposite of Conor's time so I can have positive values to look at\n",
    "adjusted_cohort_adt = pd.merge(adjusted_cohort_adt, df_admit_times, how='left', on='pat_enc_csn_id_coded')\n",
    "adjusted_cohort_adt['time_before_admit'] = adjusted_cohort_adt.apply(lambda x: x.admit_time_jittered - x.effective_time_jittered_utc, axis=1)\n",
    "adjusted_cohort_adt.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the events with some labels that make things easier later\n",
    "\n",
    "# **assumes dataframe is sorted by time\n",
    "\n",
    "# mark whether previous entry had pat class Emergency Services\n",
    "adjusted_cohort_adt['prev_emerg'] = adjusted_cohort_adt.pat_class.shift() == 'Emergency Services'\n",
    "# mark whether current event has pat class Inpatient\n",
    "adjusted_cohort_adt['curr_inpatient'] = adjusted_cohort_adt.pat_class == 'Inpatient'\n",
    "# mark whether current event is continued csn as previous\n",
    "adjusted_cohort_adt['continued'] = adjusted_cohort_adt.pat_enc_csn_id_coded.eq(\n",
    "    adjusted_cohort_adt.pat_enc_csn_id_coded.shift())\n",
    "\n",
    "# find our cases by taking the AND of these columns\n",
    "cols = ['prev_emerg', 'curr_inpatient', 'continued']\n",
    "adjusted_cohort_adt['first_ip'] = adjusted_cohort_adt[cols].all(axis=1)\n",
    "\n",
    "adjusted_cohort_adt.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first emergency services pat_class for each csn\n",
    "change_pat = adjusted_cohort_adt[~adjusted_cohort_adt.pat_class.eq(adjusted_cohort_adt.pat_class.shift())]\n",
    "change_pat.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of these still have many events - meaning they moved around a lot\n",
    "change_pat.groupby('pat_enc_csn_id_coded').jc_uid.count().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the event with pat_class == Emergency Services, that occurs before the first inpatient event\n",
    "# check to make sure that we're looking at the same CSN between both events\n",
    "# don't take any where admit ocurred before ER event - happens when people move around a lot (ER -> Inpatient -> ER -> Inpatient)\n",
    "keep_visits = change_pat[(change_pat.pat_class == 'Emergency Services') & \n",
    "                         (change_pat.shift(-1).first_ip) &\n",
    "                         (change_pat.shift(-1).pat_enc_csn_id_coded == change_pat.pat_enc_csn_id_coded) & \n",
    "                         (change_pat.time_before_admit >= timedelta(days=0))] # remove this for now\n",
    "keep_visits.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the number of csn's that were dropped becuase admit time was before first ER event\n",
    "print(hidecols)\n",
    "print(change_pat[hidecols].drop_duplicates().shape[0])\n",
    "\n",
    "print(keep_visits[hidecols].drop_duplicates().shape[0])\n",
    "\n",
    "# we lose about 300 csns where admit occurs before the first ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the time lapse between the ER visit and admission in hours\n",
    "keep_visits['hours_before_admit'] = keep_visits.time_before_admit / np.timedelta64(1, 'h')\n",
    "keep_visits.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the histogram for length of stay in ER\n",
    "keep_visits.hist('hours_before_admit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view the distribution\n",
    "print(keep_visits.hours_before_admit.describe())\n",
    "\n",
    "# how many unique csns is this?\n",
    "print(\"\\nUnique CSNs: \", keep_visits.pat_enc_csn_id_coded.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out extreme cases\n",
    "\n",
    "The longest stay in the ED was ~190 hrs (7 days). Just wanted to check out what this stay looked like to make sure nothing weird is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_visits.sort_values('hours_before_admit').drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at patient who stayed in the ER for 7 days -- seems legit\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "# grab the last row to get the extreme case\n",
    "csn = keep_visits.sort_values('hours_before_admit').iloc[-1,:].pat_enc_csn_id_coded\n",
    "adjusted_cohort_adt[adjusted_cohort_adt.pat_enc_csn_id_coded == csn].drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in a little on the histogram\n",
    "\n",
    "hours = 10\n",
    "# look at people who stayed in ER for <hours hrs\n",
    "shorter_visits = keep_visits[keep_visits.hours_before_admit < hours]\n",
    "shorter_visits.hist('hours_before_admit')\n",
    "print(shorter_visits.hours_before_admit.describe())\n",
    "\n",
    "# how many unique csns is this?\n",
    "print(\"\\nUnique CSNs: \", shorter_visits.pat_enc_csn_id_coded.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in demographics table\n",
    "\n",
    "Use the demographics table to get info about label distribution and also use the adjusted admit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from csv\n",
    "demo = pd.read_csv(\"../minh/cohort_demo.csv\")\n",
    "demo['admit_time_demo'] = demo['admit_time']\n",
    "\n",
    "# we're only keeping some of the columns from demographics\n",
    "cols = ['jc_uid', 'pat_enc_csn_id_coded', 'label', 'admit_time_demo']\n",
    "keep_joined = keep_visits.merge(demo[cols], on=['jc_uid', 'pat_enc_csn_id_coded'])\n",
    "\n",
    "# convert admit_time from demographics table into datetime\n",
    "# use this to compute time spent in ER based on this admit time\n",
    "keep_joined['admit_time_demo'] = keep_joined.admit_time_demo + '+00:00'\n",
    "keep_joined['admit_time_demo'] = pd.to_datetime(keep_joined.admit_time_demo)\n",
    "keep_joined['time_before_admit_demo'] = keep_joined.admit_time_demo - keep_joined.effective_time_jittered_utc\n",
    "# convert to hours spent in ER\n",
    "keep_joined['hours_before_admit_demo'] = keep_joined.time_before_admit_demo / np.timedelta64(1, 'h')\n",
    "keep_joined\n",
    "\n",
    "# get rid of some of the unneeded columns\n",
    "keep_joined.drop(['prev_emerg', 'curr_inpatient', 'continued', 'first_ip'], axis=1, inplace=True)\n",
    "\n",
    "hidecols = ['jc_uid', 'pat_enc_csn_id_coded']\n",
    "keep_joined.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the histogram, looks about the same as before\n",
    "keep_joined.hist('hours_before_admit_demo')\n",
    "keep_joined.hours_before_admit_demo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look into some of the extreme cases - look at most negative case since we already saw the 190 hrs case\n",
    "\n",
    "# how many cases were admitted before their first ADT ER event?\n",
    "print(\"Number of cases where admit occurred before first ER ADT event: \", \n",
    "      keep_joined[keep_joined.hours_before_admit_demo < 0].shape[0])\n",
    "\n",
    "# look into the most extreme negative case\n",
    "csn = keep_joined.sort_values('hours_before_admit_demo').iloc[0,:].pat_enc_csn_id_coded\n",
    "\n",
    "# print the admit time that we use for adjusted cohort\n",
    "print(\"\\nAdmit time for most extreme case: \", keep_joined.sort_values('hours_before_admit_demo').iloc[0,:].admit_time_demo)\n",
    "\n",
    "# looks like admit time is before the first Emergency Services - might have had more entries under a different CSN\n",
    "adjusted_cohort_adt[adjusted_cohort_adt.pat_enc_csn_id_coded == csn].drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out the distributions using the adjusted admit time\n",
    "\n",
    "Below we look at the cohort size and label distribution in the training set when we move the index time further ahead of the admit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on how index time affects the cohorts\n",
    "# this function prints out some info for the specified hours (index time)\n",
    "\n",
    "def check_index(hours):\n",
    "    print(\"** COHORT WITH INDEX TIME {} HOURS BEFORE ADMIT\".format(hours))\n",
    "    \n",
    "    # look at people who stayed in ER for >hours hrs\n",
    "    shorter_visits = keep_joined[keep_joined.hours_before_admit_demo > hours]\n",
    "\n",
    "    # how many unique csns is this?\n",
    "    unique_csn_counts = shorter_visits.pat_enc_csn_id_coded.nunique()\n",
    "    print(\"\\nUnique CSNs: \", unique_csn_counts)\n",
    "    \n",
    "    # split this into train and test\n",
    "    train = shorter_visits[shorter_visits.admit_time_demo < '2018-01-01 00:00:00+00:00']\n",
    "    test = shorter_visits[shorter_visits.admit_time_demo >= '2018-01-01 00:00:00+00:00']\n",
    "    \n",
    "    # how many unique csns is this?\n",
    "    train_csn_counts = train.pat_enc_csn_id_coded.nunique()\n",
    "    test_csn_counts = test.pat_enc_csn_id_coded.nunique()\n",
    "    print(\"\\nUnique CSNs in Training Set: \", train_csn_counts)\n",
    "    print(\"Unique CSNs in Test Set: \", test_csn_counts)\n",
    "    \n",
    "    # look into label distribution\n",
    "    train_labels = train.label.value_counts()\n",
    "    train_pos = train_labels.loc[0]\n",
    "    train_neg = train_labels.loc[1]\n",
    "    pos_perc = round(train_pos / (train_pos + train_neg) * 100, 2)\n",
    "    neg_perc = round(100 - pos_perc, 2)\n",
    "    print(\"\\nTraining Set:\\n \\\n",
    "            Negative Cases: {} ({}%)\\n \\\n",
    "            Positive Cases: {} ({}%)\\n \".format(train_pos, pos_perc, train_neg, neg_perc))\n",
    "    \n",
    "    # view the distribution\n",
    "    print(\"\\nDistribution of length of ER stays\")\n",
    "    print(\"Median: \", shorter_visits.hours_before_admit_demo.median())\n",
    "    print(shorter_visits.hours_before_admit_demo.describe())\n",
    "    \n",
    "    # VIEW HISTOGRAMS\n",
    "    # view histogram of hours since admit with lower and upper bound\n",
    "    print(\"\\n***Plotting full histogram, then histogram with upper bound at 10 hrs for better view\")\n",
    "    shorter_visits.hist('hours_before_admit_demo')\n",
    "    bound_visits = shorter_visits[shorter_visits.hours_before_admit_demo < 10]\n",
    "    bound_visits.hist('hours_before_admit_demo')\n",
    "    \n",
    "    return unique_csn_counts, train_csn_counts, test_csn_counts, train_pos, train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_index(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# condense data into a single data frame for easier comparison\n",
    "# test for a range of index times\n",
    "# this is less for the print out, and more to gather the data into the dataframe\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(10):\n",
    "    data.append(tuple([i] + list(check_index(i))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the data for different index times (hours_prior = index time is X hours before admit)\n",
    "data_df = pd.DataFrame(data, columns = ['hours_prior', 'total_unique_csn_counts', 'train_unique_csn_counts', 'test_unique_csn_counts', 'train_neg_csns', 'train_pos_csns'])\n",
    "\n",
    "data_df['train_pos_percent'] = round(data_df.train_pos_csns / (data_df.train_neg_csns + data_df.train_pos_csns) * 100, 2)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data from the dataframe\n",
    "data_df.drop('train_pos_percent', axis=1, inplace=True)\n",
    "\n",
    "# style\n",
    "plt.style.use('seaborn-darkgrid')\n",
    " \n",
    "# create a color palette\n",
    "palette = plt.get_cmap('Set1')\n",
    " \n",
    "# multiple line plot\n",
    "num=0\n",
    "for column in data_df.drop('hours_prior', axis=1):\n",
    "    num+=1\n",
    "    plt.plot(data_df['hours_prior'], data_df[column], marker='', \n",
    "             color=palette(num), linewidth=1, alpha=0.9, label=column)\n",
    " \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(\"Unique CSNs at different index times\", loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Index time: hours prior to admit\")\n",
    "    plt.ylabel(\"Number of unique CSNs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index time based on time *since* entering ER\n",
    "\n",
    "Rather than looking at an index time prior to admission to the ER, it might make more sense to look at an index time based on the time since entering the ER. This makes sense in terms of deployment because the time of admission will be unknown. \n",
    "\n",
    "Let's look at cohort stats in terms of time since entering the ER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in labs and vitals to see how these are affected by index time\n",
    "labs_vitals = pd.read_csv(\"../cohort_vitals_labs_long.csv\")\n",
    "labs_vitals.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_vitals_count = labs_vitals.groupby('pat_enc_csn_id_coded').jc_uid.count()\n",
    "print(labs_vitals_count.describe())\n",
    "labs_vitals_count.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_vitals_adt = keep_joined.merge(labs_vitals, on=['jc_uid', 'pat_enc_csn_id_coded'], how='inner')\n",
    "labs_vitals_adt.drop(['admit_time_jittered', 'time_before_admit', 'hours_before_admit', 'admit_time'], axis=1, inplace=True)\n",
    "labs_vitals_adt.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on how index time affects the cohorts\n",
    "# this function prints out some info for the specified hours (index time)\n",
    "\n",
    "def index_bounds(hours_since_ER, hours_before_admit):\n",
    "    print(\"** COHORT WITH INDEX TIME {} HOURS BEFORE ADMIT\".format(hours))\n",
    "    \n",
    "    # look at people who stayed in ER for >hours hrs\n",
    "    shorter_visits = keep_joined[keep_joined.hours_before_admit_demo > hours_before_admit]\n",
    "\n",
    "    # how many unique csns is this?\n",
    "    unique_csn_counts = shorter_visits.pat_enc_csn_id_coded.nunique()\n",
    "    print(\"\\nUnique CSNs: \", unique_csn_counts)\n",
    "    \n",
    "    # split this into train and test\n",
    "    train = shorter_visits[shorter_visits.admit_time_demo < '2018-01-01 00:00:00+00:00']\n",
    "    test = shorter_visits[shorter_visits.admit_time_demo >= '2018-01-01 00:00:00+00:00']\n",
    "    \n",
    "    # how many unique csns is this?\n",
    "    train_csn_counts = train.pat_enc_csn_id_coded.nunique()\n",
    "    test_csn_counts = test.pat_enc_csn_id_coded.nunique()\n",
    "    print(\"\\nUnique CSNs in Training Set: \", train_csn_counts)\n",
    "    print(\"Unique CSNs in Test Set: \", test_csn_counts)\n",
    "    \n",
    "    # look into label distribution\n",
    "    train_labels = train.label.value_counts()\n",
    "    train_pos = train_labels.loc[0]\n",
    "    train_neg = train_labels.loc[1]\n",
    "    pos_perc = round(train_pos / (train_pos + train_neg) * 100, 2)\n",
    "    neg_perc = round(100 - pos_perc, 2)\n",
    "    print(\"\\nTraining Set:\\n \\\n",
    "            Negative Cases: {} ({}%)\\n \\\n",
    "            Positive Cases: {} ({}%)\\n \".format(train_pos, pos_perc, train_neg, neg_perc))\n",
    "    \n",
    "    # view the distribution\n",
    "    print(\"\\nDistribution of length of ER stays\")\n",
    "    print(\"Median: \", shorter_visits.hours_before_admit_demo.median())\n",
    "    print(shorter_visits.hours_before_admit_demo.describe())\n",
    "    \n",
    "    # VIEW HISTOGRAMS\n",
    "    # view histogram of hours since admit with lower and upper bound\n",
    "    print(\"\\n***Plotting full histogram, then histogram with upper bound at 10 hrs for better view\")\n",
    "    shorter_visits.hist('hours_before_admit_demo')\n",
    "    bound_visits = shorter_visits[shorter_visits.hours_before_admit_demo < 10]\n",
    "    bound_visits.hist('hours_before_admit_demo')\n",
    "    \n",
    "    return unique_csn_counts, train_csn_counts, test_csn_counts, train_pos, train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_vitals_adt_count = labs_vitals_adt.groupby('pat_enc_csn_id_coded').jc_uid.count()\n",
    "print(labs_vitals_adt.pat_enc_csn_id_coded.nunique())\n",
    "print(labs_vitals_adt_count.describe())\n",
    "labs_vitals_adt_count.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the amount of time that's passed since vitals/labs time\n",
    "\n",
    "# change the time to datetime\n",
    "labs_vitals_adt['time'] = pd.to_datetime(labs_vitals_adt.time)\n",
    "\n",
    "labs_vitals_adt['results_elapsed_ER_time'] = labs_vitals_adt.time - labs_vitals_adt.effective_time_jittered_utc\n",
    "labs_vitals_adt['results_hours_elapsed_ER_time'] = labs_vitals_adt.results_elapsed_ER_time / np.timedelta64(1, 'h')\n",
    "labs_vitals_adt.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust some of the data frame names\n",
    "labs_vitals_adt.rename({'label': 'label_24hr',\n",
    "                        'time_before_admit_demo': 'ER_length_of_stay',\n",
    "                          'hours_before_admit_demo': 'ER_lengh_of_stay_hours'}, axis='columns', inplace=True)\n",
    "labs_vitals_adt.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into label changes based on window\n",
    "\n",
    "We want to look at different windows for the level of care change for patients. We said we'd look at 12 hours, so I'm going to grab these labels now. I'll use Conor's code that was made to create the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a fresh copy of the adjusted cohort table here\n",
    "\n",
    "# read in the cohort after saving the first time\n",
    "adj_cohort_adt_file = \"adjusted_cohort_adt.csv\"\n",
    "adjusted_cohort_adt = pd.read_csv(adj_cohort_adt_file)\n",
    "\n",
    "# change the effective time to datetime since read in from csv\n",
    "adjusted_cohort_adt.effective_time_jittered_utc = pd.to_datetime(adjusted_cohort_adt.effective_time_jittered_utc)\n",
    "\n",
    "adjusted_cohort_adt.sort_values(['pat_enc_csn_id_coded', 'seq_num_in_enc'], inplace=True)\n",
    "\n",
    "# use this to hide ID columns from view\n",
    "hidecols = ['jc_uid', 'pat_enc_csn_id_coded']\n",
    "\n",
    "\n",
    "\n",
    "# setting up variables to use conor's code\n",
    "df_change = adjusted_cohort_adt\n",
    "df_lofc = adjusted_cohort_adt\n",
    "inpatient_ids = set(adjusted_cohort_adt.pat_enc_csn_id_coded.values)\n",
    "\n",
    "# conor's code\n",
    "df_admit_times = df_change[df_change['pat_enc_csn_id_coded'].isin(inpatient_ids)]\n",
    "df_admit_times = df_admit_times[df_admit_times['pat_class'] == 'Inpatient'].groupby(\n",
    "     'pat_enc_csn_id_coded').first().reset_index()[['pat_enc_csn_id_coded', 'effective_time_jittered_utc']].rename(\n",
    "     columns={'effective_time_jittered_utc' : 'admit_time_jittered'})\n",
    "df_admit_times\n",
    "\n",
    "# Merge to df_lofc and create column called time_since_admit\n",
    "df_lofc = pd.merge(df_lofc, df_admit_times, how='left', on='pat_enc_csn_id_coded')\n",
    "df_lofc['time_since_admit'] = df_lofc.apply(lambda x: x.effective_time_jittered_utc - x.admit_time_jittered, axis=1)\n",
    "# Filter df_lofc so that we only look 12 hours into admission\n",
    "df_lofc = df_lofc[df_lofc['time_since_admit'] < timedelta(hours=12)]\n",
    "\n",
    "def was_placed_in_critical_care(arr):\n",
    "    \"\"\"Returns true if patient placed in crtical care within 24 hours of admit\n",
    "       Assumes we have already done the 24 hours logic\n",
    "       Assumes no overlapping csn ids... \"\"\"\n",
    "    for a in arr:\n",
    "        if a == 'Critical Care':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df_labels = df_lofc.groupby('pat_enc_csn_id_coded').agg({\n",
    "    'jc_uid' : 'first',\n",
    "    'admit_time_jittered' : 'first',\n",
    "    'pat_lv_of_care' : was_placed_in_critical_care}).rename(\n",
    "    columns={\"pat_lv_of_care\" : 'label'}).reset_index()[['jc_uid', 'pat_enc_csn_id_coded', 'admit_time_jittered', 'label']]\n",
    "df_labels.head()\n",
    "\n",
    "# save this to join later\n",
    "hr12_labels = df_labels\n",
    "\n",
    "df_labels.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these 12 hour labels back on to the labs_vitals_adt table\n",
    "hr12_labels['label_12hr'] = hr12_labels.label\n",
    "\n",
    "labs_vitals_adt = labs_vitals_adt.merge(hr12_labels[['pat_enc_csn_id_coded', 'label_12hr']], \n",
    "                                        on='pat_enc_csn_id_coded', how='left')\n",
    "\n",
    "# add columns to tell us how long before admission these results were recorded\n",
    "labs_vitals_adt['results_time_before_admit'] = labs_vitals_adt.admit_time_demo - labs_vitals_adt.time\n",
    "labs_vitals_adt['results_hours_before_admit'] = labs_vitals_adt.results_time_before_admit / np.timedelta64(1, 'h')\n",
    "\n",
    "labs_vitals_adt.drop(hidecols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the parts of this table that will be useful later\n",
    "labs_vitals_adt.rename({'ER_lengh_of_stay_hours': 'ER_length_of_stay_hours'}, axis='columns', inplace=True)\n",
    "cols = ['jc_uid', 'pat_enc_csn_id_coded', 'effective_time_jittered_utc', 'admit_time_demo', 'ER_length_of_stay_hours', 'results_hours_elapsed_ER_time', 'results_hours_before_admit', 'features', 'time', 'values', 'feature_type', 'label_24hr', 'label_12hr']\n",
    "\n",
    "labs_vitals_adt[cols].to_csv(\"labs_vitals_for_time_filter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check different index time constraints\n",
    "\n",
    "Now we have all of the data that we need to explore what happens with different index time constraints.\n",
    "\n",
    "We can also look at how the 12 hour window for labels affects our cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_df(df, hours_prior_to_admit, hours_in_ER, KEEP_SHORT_STAYS=0):\n",
    "    print(\"** DATA WINDOW: \\\n",
    "    {} HOURS SINCE ENTERING ER, \\\n",
    "    {} HOURS PRIOR TO ADMIT **\\n\".format(hours_in_ER, hours_prior_to_admit))\n",
    "    \n",
    "    # the KEEP_SHORT_STAYS flag allows us \n",
    "    # to look at keeping labs for patients with shorter ER stays than the \n",
    "    # hours_prior_to_admit time\n",
    "    \n",
    "    # filter this df down\n",
    "    # keep labs that occurred during the hours_in_ER time\n",
    "    df = df[df.results_hours_elapsed_ER_time <= hours_in_ER]\n",
    "    # keep labs that occur before hours_prior_to_admit\n",
    "    if KEEP_SHORT_STAYS == 0:\n",
    "        df = df[df.results_hours_before_admit >= hours_prior_to_admit]\n",
    "    else:\n",
    "        # keep labs if the ER length of stay is less than the hours_prior_to_admit time\n",
    "        df = df[(df.results_hours_before_admit >= hours_prior_to_admit) | (df.ER_length_of_stay_hours <= hours_prior_to_admit)]\n",
    "        \n",
    "    # how many unique csns do we have? These labs are already filtered to 1 hr prior to admit\n",
    "    unique_csn_counts = df.pat_enc_csn_id_coded.nunique()\n",
    "    print(\"Unique CSNs: \", unique_csn_counts)\n",
    "    \n",
    "    # how many labs/vitals per CSN do we have?\n",
    "    print(\"\\nVitals/Labs per CSN: \")\n",
    "    csn_groups = df.groupby('pat_enc_csn_id_coded').jc_uid.count()\n",
    "    print(\"median\\t \", csn_groups.median())\n",
    "    print(csn_groups.describe())\n",
    "    \n",
    "    # how many unique csns in train/test set?\n",
    "    # split this into train and test\n",
    "    train = df[df.admit_time_demo < '2018-01-01 00:00:00+00:00']\n",
    "    test = df[df.admit_time_demo >= '2018-01-01 00:00:00+00:00']\n",
    "\n",
    "    train_csn_counts = train.pat_enc_csn_id_coded.nunique()\n",
    "    test_csn_counts = test.pat_enc_csn_id_coded.nunique()\n",
    "    print(\"\\nUnique CSNs in TRAIN: \", train_csn_counts)\n",
    "    print(\"Unique CSNs in TEST: \", test_csn_counts)\n",
    "    \n",
    "    # how many labs/vitals per CSN in TRAIN?\n",
    "    print(\"\\nVitals/Labs per CSN in TRAIN Set: \")\n",
    "    csn_groups = train.groupby('pat_enc_csn_id_coded').jc_uid.count()\n",
    "    train_labs_vitals_median = csn_groups.median()\n",
    "    print(\"median\\t \", train_labs_vitals_median)\n",
    "    print(csn_groups.describe())\n",
    "    \n",
    "    # how many labs per CSN?\n",
    "    type_counts = train.groupby(['feature_type', 'pat_enc_csn_id_coded']).jc_uid.count()\n",
    "    print(\"\\nLabs per CSN in TRAIN set: \")\n",
    "    train_labs_median = type_counts.labs.median()\n",
    "    print(\"median\\t \", train_labs_median)\n",
    "    print(type_counts.labs.describe())\n",
    "        \n",
    "    # how many vitals per CSN?\n",
    "    print(\"\\nVitals per CSN in TRAIN set: \")\n",
    "    train_vitals_median = type_counts.vitals.median()\n",
    "    print(\"median\\t \", train_vitals_median)\n",
    "    print(type_counts.vitals.describe())\n",
    "\n",
    "    # what's our label distribution?\n",
    "    train_labels_24hr = train.label_24hr.value_counts()\n",
    "    train_pos_24hr = train_labels_24hr.loc[1]\n",
    "    train_neg_24hr = train_labels_24hr.loc[0]\n",
    "    pos_perc = round(train_pos_24hr / (train_pos_24hr + train_neg_24hr) * 100, 2)\n",
    "    neg_perc = round(100 - pos_perc, 2)\n",
    "    print(\"\\nTraining Set (24 hr labels):\\n \\\n",
    "            Negative Cases: {} ({}%)\\n \\\n",
    "            Positive Cases: {} ({}%)\\n \".format(train_neg_24hr, neg_perc, train_pos_24hr, pos_perc))\n",
    "    \n",
    "    train_labels_12hr = train.label_12hr.value_counts()\n",
    "    train_pos_12hr = train_labels_12hr.loc[1]\n",
    "    train_neg_12hr = train_labels_12hr.loc[0]\n",
    "    pos_perc = round(train_pos_12hr / (train_pos_12hr + train_neg_12hr) * 100, 2)\n",
    "    neg_perc = round(100 - pos_perc, 2)\n",
    "    print(\"\\nTraining Set (12hr labels):\\n \\\n",
    "            Negative Cases: {} ({}%)\\n \\\n",
    "            Positive Cases: {} ({}%)\\n \".format(train_neg_12hr, neg_perc, train_pos_12hr, pos_perc))\n",
    "    \n",
    "    return (unique_csn_counts, train_csn_counts, test_csn_counts, \n",
    "            train_labs_vitals_median, train_labs_median, train_vitals_median,\n",
    "            train_pos_24hr, train_neg_24hr, train_pos_12hr, train_neg_12hr)\n",
    "    \n",
    "hours_prior_to_admit = 1\n",
    "hours_in_ER = 1\n",
    "describe_df(labs_vitals_adt, hours_prior_to_admit, hours_in_ER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hours_prior_to_admit = 1\n",
    "hours_in_ER = 1\n",
    "\n",
    "data = []\n",
    "for hours_prior_to_admit in range(1,10):\n",
    "    for hours_in_ER in range(1,10):\n",
    "        data.append(tuple([hours_prior_to_admit, hours_in_ER] + list(describe_df(labs_vitals_adt, hours_prior_to_admit, hours_in_ER))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = 'hours_prior_to_admit, hours_in_ER, \\\n",
    "            unique_csn_counts, train_csn_counts, test_csn_counts, \\\n",
    "            train_labs_vitals_median, train_labs_median, train_vitals_median,\\\n",
    "            train_pos_24hr, train_neg_24hr, train_pos_12hr, train_neg_12hr'\n",
    "cols = cols.split(', ')\n",
    "cols = [x.strip() for x in cols]\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=cols)\n",
    "data_df.to_csv('index_time_lab_vitals.csv', index=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the unique csn counts\n",
    "cols = ['hours_prior_to_admit', 'hours_in_ER', 'unique_csn_counts']\n",
    "csns = pd.DataFrame(data_df[cols])\n",
    "\n",
    "# let's look at these in 10,000's\n",
    "csns.unique_csn_counts = csns.unique_csn_counts / 10000\n",
    "\n",
    "csns = csns.pivot(index='hours_prior_to_admit', columns='hours_in_ER', values='unique_csn_counts')\n",
    "csns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like hours in ER doesn't have much effect on the number of unique CSNs we include\n",
    "def simple_spaghetti(df, title, xlab, ylab):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    # style\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('tab10')\n",
    "\n",
    "    # multiple line plot\n",
    "    num=0\n",
    "    for column in df:\n",
    "        num+=1\n",
    "        plt.plot(df.index, df[column], marker='', color=palette(num), linewidth=5, alpha=0.9, label=column)\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(title='Hours in ER')\n",
    "\n",
    "        # Add titles\n",
    "        plt.title(title, loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "        plt.xlabel(xlab)\n",
    "        plt.ylabel(ylab)\n",
    "\n",
    "        \n",
    "title = 'Unique CSN counts with index time changes'\n",
    "xlab = 'Hours prior to Admit'\n",
    "ylab = 'Number of Unique CSNs'\n",
    "simple_spaghetti(csns, title, xlab, ylab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the unique train_labs_vitals_median counts\n",
    "cols = ['hours_prior_to_admit', 'hours_in_ER', 'train_labs_vitals_median']\n",
    "labs_vitals = pd.DataFrame(data_df[cols])\n",
    "\n",
    "labs_vitals = labs_vitals.pivot(index='hours_prior_to_admit', columns='hours_in_ER', values='train_labs_vitals_median')\n",
    "labs_vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I thought it was odd to see this v-shape in the plots below, but I think it does make sense.\n",
    "\n",
    "We'd expect that the number of labs/vitals should increase as hours in the ER increases, because there is more time to collect more labs/vitals. \n",
    "\n",
    "We'd expect that the number of labs/vitals should decrease as the hours prior to admit increases. We're essentially shortening the available window to collect these labs/vitals.\n",
    "\n",
    "In combination, as we increase the hours prior to admit, we esentially exclude patients who have been in the ER for less than that window time. This means our cohort is essentially biased towards people with longer stays in the ER. As such, the median number of labs increases.\n",
    "\n",
    "Probably want to look into keeping cases where the entire length of stay in the ER < time prior to admit that we use to prevent data leakage. Maybe we can keep this at a minimum of 1 hour prior to admit though since our early SQL included this and so all data tables afterwards also do not contain any of these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not many labs are available within one hour of entering ER, 2 hours+ are all pretty similar\n",
    "\n",
    "title = 'Labs/vitals available within window (TRAIN SET)'\n",
    "xlab = 'Hours prior to admit'\n",
    "ylab = 'Median number of labs/vitals available'\n",
    "simple_spaghetti(labs_vitals, title, xlab, ylab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the training labels\n",
    "cols = ['hours_prior_to_admit', 'hours_in_ER', 'train_pos_24hr', 'train_neg_24hr', 'train_pos_12hr', 'train_neg_12hr']\n",
    "labels = pd.DataFrame(data_df[cols])\n",
    "\n",
    "# add some percentages that we're going to plot\n",
    "labels['percent_pos_24hr'] = round(labels.train_pos_24hr / (labels.train_pos_24hr + labels.train_neg_24hr) * 100,2)\n",
    "labels['percent_pos_12hr'] = round(labels.train_pos_12hr / (labels.train_pos_12hr + labels.train_neg_12hr) * 100,2)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the labels separately\n",
    "cols = ['hours_prior_to_admit', 'hours_in_ER', 'percent_pos_24hr']\n",
    "labels_24hr = pd.DataFrame(labels[cols])\n",
    "\n",
    "labels_24hr = labels_24hr.pivot(index='hours_prior_to_admit', columns='hours_in_ER', values='percent_pos_24hr')\n",
    "labels_24hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Positive 24hr labels within window'\n",
    "xlab = 'Hours prior to admit'\n",
    "ylab = 'Percent positive labels in training cohort'\n",
    "simple_spaghetti(labels_24hr, title, xlab, ylab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 12 hour labels\n",
    "cols = ['hours_prior_to_admit', 'hours_in_ER', 'percent_pos_12hr']\n",
    "labels_12hr = pd.DataFrame(labels[cols])\n",
    "\n",
    "labels_12hr = labels_12hr.pivot(index='hours_prior_to_admit', columns='hours_in_ER', values='percent_pos_12hr')\n",
    "labels_12hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Positive 12hr labels within window'\n",
    "xlab = 'Hours prior to admit'\n",
    "ylab = 'Percent positive labels in training cohort'\n",
    "simple_spaghetti(labels_12hr, title, xlab, ylab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping patients with shorter stays than the \"hours prior to admit\" value\n",
    "\n",
    "The labs counts looked a little weird with that v-shape earlier so now we'll check what happens when we keep patient's labs if their ER stay was shorter than the hours_prior_to_admit time. \n",
    "\n",
    "This is what we originally said we would do with the shorter stays. \n",
    "\n",
    "*Note* These still don't include any labs that occurr within one hour of admit because those were filtered out using SQL to create a table used in a earlier step. We need to change the original table if we want to get those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keepshort = []\n",
    "for hours_prior_to_admit in range(1,10):\n",
    "    for hours_in_ER in range(1,10):\n",
    "        data_keepshort.append(tuple([hours_prior_to_admit, hours_in_ER] \n",
    "                          + list(describe_df(labs_vitals_adt, hours_prior_to_admit, hours_in_ER, KEEP_SHORT_STAYS=1))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'hours_prior_to_admit, hours_in_ER, \\\n",
    "            unique_csn_counts, train_csn_counts, test_csn_counts, \\\n",
    "            train_labs_vitals_median, train_labs_median, train_vitals_median,\\\n",
    "            train_pos_24hr, train_neg_24hr, train_pos_12hr, train_neg_12hr'\n",
    "cols = cols.split(', ')\n",
    "cols = [x.strip() for x in cols]\n",
    "\n",
    "data_keepshort_df = pd.DataFrame(data_keepshort, columns=cols)\n",
    "data_keepshort_df.to_csv('index_time_lab_vitals_keepshort.csv', index=False)\n",
    "data_keepshort_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the unique train_labs_vitals_median counts\n",
    "cols = ['hours_prior_to_admit', 'hours_in_ER', 'train_labs_vitals_median']\n",
    "labs_vitals = pd.DataFrame(data_keepshort_df[cols])\n",
    "\n",
    "labs_vitals = labs_vitals.pivot(index='hours_prior_to_admit', columns='hours_in_ER', values='train_labs_vitals_median')\n",
    "labs_vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not many labs are available within one hour of entering ER, 2 hours+ are all pretty similar\n",
    "\n",
    "title = 'Labs/vitals available within window (TRAIN SET)'\n",
    "xlab = 'Hours prior to admit'\n",
    "ylab = 'Median number of labs/vitals available'\n",
    "simple_spaghetti(labs_vitals, title, xlab, ylab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables that will allow for easily choosing the labs/vitals that we need\n",
    "\n",
    "Will connect these with the traige_cohort_adjusted_vitals_labs_4binning so that they can be easily filtered based on whatever time points we decide.\n",
    "\n",
    "Also, store the labels for the CSNs for easy use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bigquery triage_cohort_adjusted_vitals_labs_4binning\n",
    "# select *\n",
    "# from traige_TE.triage_cohort_adjusted_vitals_labs_4binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triage_cohort_adjusted_vitals_labs_4binning.to_csv(\"triage_cohort_adjusted_vitals_labs_4binning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bigquery triage_cohort_adjusetd_demographics_clean\n",
    "# select *\n",
    "# from traige_TE.triage_cohort_adjusted_demographics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triage_cohort_adjusetd_demographics_clean.to_csv(\"triage_cohort_adjusetd_demographics_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_cohort_adjusetd_demographics_clean = pd.read_csv(\"triage_cohort_adjusetd_demographics_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labs_vitals_adt[['jc_uid', 'pat_enc_csn_id_coded', 'label_24hr', 'label_12hr']]\n",
    "labels = labels.drop_duplicates(keep='first')\n",
    "\n",
    "joined_demo = triage_cohort_adjusetd_demographics_clean.merge(labels,\n",
    "                                                             on=['jc_uid', 'pat_enc_csn_id_coded'],\n",
    "                                                             how='left')\n",
    "\n",
    "# we're missing about 3,073 CSNs here - these should be the patients who were dropped because\n",
    "# their admit time was before their first ER visit\n",
    "joined_demo[(joined_demo.label != joined_demo.label_24hr) & \n",
    "           (joined_demo.label_24hr.isnull())][hidecols].drop_duplicates().shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
