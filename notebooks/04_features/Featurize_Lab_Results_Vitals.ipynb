{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "### THIS IS MEANT TO RUN ON NERO - NEEDS TO BE CHANGED IF YOU RUN LOCALLY\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/ccorbin/.config/gcloud/application_default_credentials.json' \n",
    "os.environ['GCLOUD_PROJECT'] = 'mining-clinical-decisions' \n",
    "%load_ext google.cloud.bigquery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client=bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from traige_TE.triage_features_demos_vitals_labs\"\"\"\n",
    "query_job = client.query(query)\n",
    "df = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['feature_type'].isin(['labs', 'vitals'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Continuous Features\n",
    "Create a function that \"trains\" binning featurizer (computes distribution of values) based on subset of the data.  This is important because we only want to build the distribution with our training set and apply the bin featurizer\n",
    "to the test set (prevents leakage). \n",
    "\n",
    "Then create a function that \"applies\" the trained featurizer on a set of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_dict(look_up_table):\n",
    "    \"\"\"Converts df look up table to dictionary for faster look up later\"\"\"\n",
    "    bin_val_dict = {}\n",
    "    for feature in look_up_table['features'].unique():\n",
    "        feature_bin_vals = look_up_table[look_up_table['features'] == feature]\n",
    "        for _bin in feature_bin_vals['bins'].unique():\n",
    "            if feature not in bin_val_dict:\n",
    "                bin_val_dict[feature] = {}\n",
    "                bin_val_dict[feature]['min'] = []\n",
    "                bin_val_dict[feature]['max'] = []\n",
    "\n",
    "            min_val_for_bin = feature_bin_vals[feature_bin_vals['bins'] == _bin]['values']['min'].values[0]\n",
    "            max_val_for_bin = feature_bin_vals[feature_bin_vals['bins'] == _bin]['values']['max'].values[0]\n",
    "\n",
    "            bin_val_dict[feature]['min'].append(min_val_for_bin)\n",
    "            bin_val_dict[feature]['max'].append(max_val_for_bin)\n",
    "    return bin_val_dict\n",
    "\n",
    "    \n",
    "def train_featurizer(df_train):\n",
    "    \"\"\"\n",
    "    Compute percent_ranks and generates a look up table of min and max bin values\n",
    "    Input : long form dataframe with features and values where values are the continuous values of labs / vitals\n",
    "    Output: look up table - dict of dict of lists (key1 = feature_name, key2 = max or min, values = lists of values)\n",
    "    \"\"\"\n",
    "    # Compute percentiles and bins\n",
    "    df_train['percentiles'] = df_train.groupby('features')['values'].transform(lambda x: x.rank(pct=True))\n",
    "    df_train['bins'] = df_train['percentiles'].apply(lambda x: int(x * 10))\n",
    "    \n",
    "    # Generate look up table and conver to dictionary stucture\n",
    "    look_up_table_df = df_train.groupby(['features', 'bins']).agg({'values' : ['min', 'max']}).reset_index()\n",
    "    look_up_table = convert_to_dict(look_up_table_df)\n",
    "    \n",
    "    ### Sanity Check. Ensure that min vector for each feature is strictly increasing (no ties!)\n",
    "    # Should be the case because ties are given same percentile rank in default pandas rank function\n",
    "    for feature in look_up_table:\n",
    "        mins = look_up_table[feature]['min']\n",
    "        for i in range(len(mins)-1):\n",
    "            assert mins[i] < mins[i+1]\n",
    "    \n",
    "    return look_up_table\n",
    "\n",
    "\n",
    "def apply_featurizer(df, look_up_table):\n",
    "    \n",
    "    def get_appropriate_bin(feature, value, look_up_table):\n",
    "        \"\"\"Takes in feature, value and look up table and returns appropriate bin\n",
    "\n",
    "        Quick Note: For some features, we do not have 10 bins.  This happens when we have many many ties in the \n",
    "        percent rank - and the percent rank alg returns ties as the average rank within that tie. So for instance\n",
    "        we're trying to break each feature up into deciles where each bin covers range of 10% of the examples. But if more\n",
    "        than 10% of the examples take on 1 value - then bins can be skipped. This shouldn't really be a problem\n",
    "        for downstream tasks - just something to be aware of. This also means 'bins' and 'bins_applied' won't have\n",
    "        perfect overlap in features that end up having less than 10 bins\n",
    "\n",
    "        \"\"\"\n",
    "        mins = look_up_table[feature]['min']\n",
    "        for i in range(len(mins) - 1):\n",
    "            # If value is smaller than min value of smallest bin (in test time) - then return 0 (smallest bin)\n",
    "            if i == 0 and value < mins[i]:\n",
    "                return i\n",
    "\n",
    "            if value >= mins[i] and value < mins[i+1] :\n",
    "                return i\n",
    "\n",
    "        # Then in last bin\n",
    "        return len(mins)-1\n",
    "    \n",
    "    df['bins_applied'] = df[['features', 'values']].apply(\n",
    "        lambda x: get_appropriate_bin(x['features'], x['values'], look_up_table), axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train And Apply Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['admit_time'] < '2018-01-01']\n",
    "\n",
    "look_up_table = train_featurizer(df_train)\n",
    "df_featurized = apply_featurizer(df, look_up_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sanity Check\n",
    "For features that have 10 bins from 0 to 9 - `bins` should be same as `bins_applied`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = apply_featurizer(df_train, look_up_table)\n",
    "look_up_table_df = df_train.groupby(['features', 'bins']).agg({'values' : ['min', 'max']}).reset_index()\n",
    "\n",
    "features_with_0_9_bins = []\n",
    "for feature in look_up_table_df:\n",
    "    num_bins = len(look_up_table_df[look_up_table_df['features'] == feature]['bins'].values)\n",
    "    ten_in_bins = 10 in look_up_table_df[look_up_table_df['features'] == feature]['bins'].values\n",
    "    if num_bins == 10 and not ten_in_bins:\n",
    "        features_with_0_9_bins.append(feature)\n",
    "\n",
    "for feature in features_with_0_9_bins:\n",
    "    df_test = df_train[df_train['features'] == 'feature']\n",
    "    for b_real, b_computed in zip(df_test['bins'].values, df_test['bins_applied'].values):\n",
    "        assert(b_real == b_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bit of house cleaning\n",
    "Create new feature names that reflect which bin the value belongs in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['anon_id', 'pat_enc_csn_id_coded', 'admit_time', 'feature_type', 'features', 'values', 'bins_applied']\n",
    "df_new = df_featurized[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['features'] = ['_'.join([x, str(y)]) for x, y in zip(df_new['features'].values, df_new['bins_applied'].values)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Counts representation\n",
    "Group by patient, cns, and feature name (with bin value appended to feature name) and make value the number of times\n",
    "that particular feature appears for that csn id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_new.groupby(['anon_id', 'pat_enc_csn_id_coded', 'features']).agg(\n",
    "    {'admit_time' : 'first',\n",
    "     'feature_type' : 'first',\n",
    "     'values' : 'count'}).reset_index()\n",
    "\n",
    "columns = ['anon_id', 'pat_enc_csn_id_coded', 'admit_time', 'feature_type', 'features', 'values']\n",
    "df_final = df_final[columns] # reorder columns\n",
    " \n",
    "# Rename feature_type to reflect training set used.  'vitals_test' means everything up to 2018 used. (train + dev)\n",
    "# 'vitals_train' means everything up to July 2017 used. (train)\n",
    "df_final['feature_type'] = [x + '_results' if x == 'labs' else x for x in df_final['feature_type'].values]\n",
    "df_final['feature_type'] = [x + '_train' for x in df_final['feature_type'].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - sum of the counts should be length of the orginal dataframe\n",
    "assert df_final['values'].sum() == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV until I can get my own custom env on nero to upload to big query from here\n",
    "df_final.to_csv('bins_labs_vitals_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in look_up_table_df['features'].unique():\n",
    "    print(look_up_table_df[look_up_table_df['features'] == feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
